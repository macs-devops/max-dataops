sidebar:
    position: right
    about: True # how to use.
    education: True # in sidebar

    name: Max Florentin
    tagline: Data & DevOps
    avatar: profile.jpg

    email: macsee13@gmail.com
    citizenship:
    website: www.dataops.com.ar
    linkedin: florentin-m
    github: maxflorentin
    telegram: # add your nickname without '@' sign
    stack-overflow: # Number/Username, e.g. 123456/alandoe
    pdf: https://drive.google.com/file/d/1H3GuearwYnKA7oIHcJI6C-QHt27nYKgA/view?usp=sharing

    languages:
      title: Languages
      info:
        - idiom: English
          level: Intermediate

        - idiom: Spanish
          level: Native

    interests:
      title: Interests
      info:
        - item: Tech
          link:

        - item: Agriculture
          link:

        - item: Cooking
          link:

career-profile:
    title: Career Profile
    summary: |
      With over 14 years of experience spanning BI development, Data Engineering, DataOps, and DevOps, I've developed a comprehensive understanding of the data industry's ecosystem. 
      My diverse roles have enabled me to master the complete data lifecycle—from initial collection and processing to advanced analytics and model deployment. 
      I've consistently delivered solutions to complex real-world problems, collaborating effectively across teams and technical domains. 
      
      This broad expertise allows me to see beyond individual components to understand how each element contributes to a project's success. 
      I pride myself on being a proactive team player who combines technical proficiency with a growth mindset, always eager to tackle new challenges and expand my skillset.
education:
    title: Education
    info:
      - degree: Certificacion Universitaria en DevOps
        university: Universidad Nacional de Cordoba
        time: 2021 - 2022
        details: |
          Through my university DevOps certification, I have gained a deep understanding of DevOps principles, such as continuous delivery, 
          continuous integration, and DevOps culture. I have also learned about the tools and technologies that are used to implement these principles. 
          I am confident that this training will help me succeed in a DevOps career.
            - DevOps Culture
            - Git
            - Build & Package Managers
            - CI/CD
            - Containers
            - Cloud Services
            - IaC
            - Telemetry 
      - degree: Diplomatura Arquitecturas en Big Data Aplicadas 
        university: UTN
        time: 2019 - 2020
        details: |
          Covered fundamental technologies and methodologies for large-scale data processing and analysis. 
          The program provided comprehensive coverage of the Apache Hadoop ecosystem for distributed computing, along with robust training in NoSQL databases including MongoDB, Cassandra, and Neo4j, each oriented toward different use cases and data architectures. 
          The curriculum delved into distributed programming for efficient processing of large data volumes, as well as advanced Text Mining techniques for analyzing and extracting information from unstructured data. 
          The program culminated with an integrated approach to Data Science, encompassing statistical methods and analytical techniques for extracting meaningful insights from complex datasets.
      - degree: Systems Analyst
        university: IRSO
        time: 2018 - 2020
        details: |
          I dropped out of the second year of the Systems Analyst career. I had to focus on other priorities at the time, 
          but I am still interested in pursuing a career in systems analysis. 
          I am currently working on my own projects to build my skills and knowledge.

experiences:
    title: Experiences
    info:
      - role: Tech Lead DataOps Engineering
        time: 2023 - Present
        company: dlocal, Uruguay
        details: |
          Founded and led a new DataOps team, responsible for building a platform for data teams.
          Deployed Apache Airflow first on Amazon Elastic Container Service (ECS) and then on AWS EKS. 
          The deployment uses GitHub Actions to automate the deployment process and Argo CD.
          Created a custom DAG factory to automate the creation of DAGs from YAML files.
          Customized operators, plugins, and connections to allow jobs to be executed
          from the AWS account of Data in other accounts of the organization.
          Designed a new data lake (Apache Iceberg) simplify data pipelines and optimize rythm of ingestions (near real time) and reduce costs. 
            - Founded team
            - Design architectures for Data solutions
      - role: Senior Data Engineer
        time: 2022 - 2023
        company: dlocal, Uruguay
        details: |
            - Collaborate with legacy system maintenance
      - role: Senior Staff DataOps Engineer
        time: 2022 - 2022
        company: ank, Argentina
        details: |
          During my time at the company as a DataOps, I spearheaded the implementation of tools and 
          processes aimed at streamlining daily tasks for data engineers. 
          This included optimizing build processes, deployments, enhancing observability, and improving incident management. 
          Also was a DevOps role, I participated in troubleshooting and deployments collaborating with the team.
            - Terraform modules for data streams
            - Improve performance Metabase cluster.
      - role: Senior Staff Data Engineer
        time: 2021 - 2022
        company: ank, Argentina
        details: |
          I led the building of data platform, conducting analyses on tools and technologies for delivery to diverse user
          groups, including analytics and operations teams. My responsibilities extended to architecting the data lake and 
          databases, implementing pipelines and automations. 
          Additionally, I played a key role in deploying, administering, and providing support for the team's infrastructure.
            - Apache Airflow running on k8s
            - Redshift Materialized Views improvemets
            - Apache Airflow - EMR steps integrations and DAG factory.

certifications:
      title: Certifications
      list:
        - name: MS-AI-050
          start: 2023
          end: 2023
          organization: IT College
          credentialid: 88359912
          credentialurl: "credentials.itcollege.com.ar/6ba5a130-4072-4fd0-8538-ade50a3c1925#gs.1cjpmk"
          credentialname: Develop of AI solutions with Azure OpenAI Service 
        - name: AWS Data Engineering
          start: 2023
          end: 2023
          organization: Datapath
          credentialid: DATAPATH-CE-033-2023-010
          credentialurl: "datapath.iacerts.com/usuario/certificado/9c998ebe-a5d8-44e5-af0e-eca131d0a0b6"
          credentialname: AWS Data Engineering

projects:
    title: Projects
    intro: >
      Some projects in which I collaborated in parallel to my main activity.
    assignments:
      - title: Panamá Wallet
        link: "#hook"
        tagline: "I participated as a Lead Data Engineering and as a DevOps"

      - title: Lahuen Cooperativa de Trabajo
        link: "https://www.lahuen.ar/"
        tagline: "Founder and CTO"

      - title: Coderhouse
        link: "https://www.coderhouse.com"
        tagline: "Classes, tutorials and talks as a Data specialist"

skills:
    title: Skills &amp; Proficiency

    toolset:
      - name: Python, SQL & Bash
        level: 98%

      - name: Apache Airflow
        level: 98%

      - name: Git
        level: 98%

      - name: IaC
        level: 95%

      - name: Cloud (AWS)
        level: 85%

      - name: Docker & k8s 
        level: 70%

      - name: CI/CD
        level: 80%

footer: >
    Designed with <i class="fas fa-heart"></i> by Max</a>
